{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(path: str):\n",
    "    chunks = []\n",
    "    with open(path) as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                chunks.append(line)\n",
    "    return chunks\n",
    "\n",
    "meditations = '../data/meditations.txt'\n",
    "chunks = load_sentences(meditations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/garrett.partenza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/garrett.partenza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrett.partenza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(chunk: str):\n",
    "    chunk = chunk.lower()\n",
    "    chunk = re.sub(r'[^a-z\\s]', '', chunk)\n",
    "    tokens = word_tokenize(chunk)\n",
    "    cleaned_tokens = [\n",
    "        word for word in tokens\n",
    "        if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    cleaned_chunk = ' '.join(cleaned_tokens)\n",
    "    return cleaned_chunk\n",
    "\n",
    "chunks_clean = list(clean_text(chunk) for chunk in chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, KeyedVectors\n",
    "\n",
    "corpus = list(chunk.split() for chunk in chunks_clean)\n",
    "\n",
    "model = FastText(\n",
    "    corpus,\n",
    "    vector_size=256,\n",
    "    window=8,\n",
    "    min_count=1,\n",
    "    sg=0,\n",
    "    workers=4,\n",
    "    bucket=100000\n",
    ")\n",
    "\n",
    "model.save(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"test\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_chunk(chunk: str, model):\n",
    "    embeddings = list(model.wv[word].astype(np.float64) for word in chunk.split())\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "\n",
    "model = FastText.load(\"word2vec_model\")\n",
    "vectors = list(embed_chunk(chunk, model) for chunk in chunks_clean)\n",
    "print(vectors[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_clean</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From my grandfather Verus I learned good moral...</td>\n",
       "      <td>grandfather verus learned good morals governme...</td>\n",
       "      <td>[-0.2031693011522293, 0.26146584323474337, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From the reputation and remembrance of my fath...</td>\n",
       "      <td>reputation remembrance father modesty manly ch...</td>\n",
       "      <td>[-0.26931800693273544, 0.3459737226366997, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From my mother, piety and beneficence, and abs...</td>\n",
       "      <td>mother piety beneficence abstinence evil deeds...</td>\n",
       "      <td>[-0.22908581979572773, 0.29476446146145463, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From my great-grandfather, not to have frequen...</td>\n",
       "      <td>greatgrandfather frequented public schools goo...</td>\n",
       "      <td>[-0.22445550902436176, 0.28823872407277423, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From my governor, to be neither of the green n...</td>\n",
       "      <td>governor neither green blue party games circus...</td>\n",
       "      <td>[-0.18980855967562932, 0.244022560520814, 0.08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  From my grandfather Verus I learned good moral...   \n",
       "1  From the reputation and remembrance of my fath...   \n",
       "2  From my mother, piety and beneficence, and abs...   \n",
       "3  From my great-grandfather, not to have frequen...   \n",
       "4  From my governor, to be neither of the green n...   \n",
       "\n",
       "                                         chunk_clean  \\\n",
       "0  grandfather verus learned good morals governme...   \n",
       "1  reputation remembrance father modesty manly ch...   \n",
       "2  mother piety beneficence abstinence evil deeds...   \n",
       "3  greatgrandfather frequented public schools goo...   \n",
       "4  governor neither green blue party games circus...   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.2031693011522293, 0.26146584323474337, 0.0...  \n",
       "1  [-0.26931800693273544, 0.3459737226366997, 0.1...  \n",
       "2  [-0.22908581979572773, 0.29476446146145463, 0....  \n",
       "3  [-0.22445550902436176, 0.28823872407277423, 0....  \n",
       "4  [-0.18980855967562932, 0.244022560520814, 0.08...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "database = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"chunk\": chunks,\n",
    "        \"chunk_clean\": chunks_clean,\n",
    "        \"vector\": vectors\n",
    "    }\n",
    ")\n",
    "\n",
    "database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatabase\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeditations.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'database' is not defined"
     ]
    }
   ],
   "source": [
    "database[[\"chunk\", \"vector\"]].to_csv(\"meditations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe planet\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      7\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m embed_chunk(clean_text(query), model)\n\u001b[0;32m----> 9\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m cosine_similarity([query_vector], \u001b[43mdatabase\u001b[49m\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mto_list())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m similarity_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(database\u001b[38;5;241m.\u001b[39mchunk, similarity_scores))\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(similarity_pairs, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'database' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"universe and planet\"\n",
    "print(clean_text(query))\n",
    "top_k = 3\n",
    "\n",
    "query_vector = embed_chunk(clean_text(query), model)\n",
    "\n",
    "similarity_scores = cosine_similarity([query_vector], database.vector.to_list())[0]\n",
    "similarity_pairs = list(zip(database.chunk, similarity_scores))\n",
    "results = sorted(similarity_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_k_results = []\n",
    "for k in range(top_k):\n",
    "    print(results[k][0])\n",
    "    top_k_results.append(results[k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_k_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m\n\u001b[1;32m     45\u001b[0m     response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m     46\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3:instruct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     47\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m system_prompt, user_prompt \u001b[38;5;241m=\u001b[39m generate_prompts(query, \u001b[43mtop_k_results\u001b[49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(stoic_guide(system_prompt, user_prompt))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_k_results' is not defined"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_prompts(query, results):\n",
    "    system_prompt = (\n",
    "        \"<system_prompt>\"\n",
    "        \"You are a Stoic AI assistant, deeply versed in the teachings of Marcus Aurelius. \"\n",
    "        \"Your job is to follow the user's task exactly, not straying from any of the directions provided to you.\"\n",
    "        \"</system_prompt>\"\n",
    "    )\n",
    "\n",
    "    task_xml = (\n",
    "        \"<task>\"\n",
    "        \"Analyze the following user query and the provided quotes from Marcus Aurelius' Meditations. \"\n",
    "        \"Select the most relevant quote that addresses the user's concern. Structure your response as follows:\"\n",
    "        \"<instructions>\"\n",
    "        \"<step>Quote: Begin with the chosen quote, enclosed in quotation marks.</step>\"\n",
    "        \"<step>Do not hallucinate the chosen quote, you must choose one from the given results.</step>\"\n",
    "        \"<step>Interpretation: In 2-3 sentences, why you chose this quote, given the users original query.</step>\"\n",
    "        \"<step>Write from the point of view that, the user is trusting that this is the most relevant quote.</step>\"\n",
    "        \"<step>Advice: In 4-5 sentences, offer practical guidance based on the quote and Stoic principles.</step>\"\n",
    "        \"<step>Do not write more than a few sentences outside of the selected quote.</step>\"\n",
    "        \"<step>Do not discuss anything about stoicism outside of the quote and query.</step>\"\n",
    "        \"</instructions>\"\n",
    "        \"Maintain a wise and compassionate tone throughout your response. Aside from citing your chosen quote, use language that assumes you are speaking to the original user personally. Use language and style that mirrors that of a modern day philosopher spreading stoic wisdom to a student. Construct your response in parsable XML format with <quote>, <interpretation>, and <advice> for the keys mentioned in the afformentioned steps, including a <root> key for the root of the entire response.\"\n",
    "        \"</task>\"\n",
    "    )\n",
    "\n",
    "    query_xml = f\"<rag_query>{query}</rag_query>\"\n",
    "\n",
    "    search_results_xml = \"<search_results>\" + \"\".join(\n",
    "        f\"<search_result>{result}</search_result>\" for result in results\n",
    "    ) + \"</search_results>\"\n",
    "\n",
    "    user_prompt = f\"<user_prompt>{task_xml}{query_xml}{search_results_xml}</user_prompt>\"\n",
    "\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "def stoic_guide(system_prompt, user_prompt):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3:instruct',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "system_prompt, user_prompt = generate_prompts(query, top_k_results)\n",
    "print(stoic_guide(system_prompt, user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext_inner import ft_hash_bytes\n",
    "import json\n",
    "\n",
    "def custom_ft_hash_bytes(bytez: bytes) -> int:\n",
    "    h = 2166136261\n",
    "    for b in bytez:\n",
    "        h = h ^ b  # XOR the current byte value\n",
    "        h = h * 16777619  # Multiply by the magic prime number\n",
    "    return h & 0xFFFFFFFF  # Ensure the result is bounded to 32 bits\n",
    "\n",
    "def generate_char_ngrams(text, n):\n",
    "    \"\"\"\n",
    "    Generates character n-grams from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        n (int): The length of the n-grams.\n",
    "    Returns:\n",
    "        list: A list of character n-grams.\n",
    "    \"\"\"\n",
    "    ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "def generate_ngrams(word, min=3, max=6):\n",
    "    ngram_lists = list(generate_char_ngrams(word, x) for x in range(min, max+1))\n",
    "    ngrams = [item for sublist in ngram_lists for item in sublist]\n",
    "    return ngrams\n",
    "\n",
    "def custom_embed(word: str):\n",
    "    if word in model.wv.key_to_index:\n",
    "        print(\"Whole word found\")\n",
    "        return model.wv.vectors[model.wv.key_to_index[word]]\n",
    "    ngrams = generate_ngrams(\"<\"+word+\">\")\n",
    "    res = np.zeros(256)\n",
    "    for ngram in ngrams:\n",
    "        bytez = ngram.encode('utf-8')\n",
    "        hash_value = ft_hash_bytes(bytez)\n",
    "        bounded_hash_value = hash_value % 100000\n",
    "        vec = model.wv.vectors_ngrams[bounded_hash_value]\n",
    "        res += vec\n",
    "    return res / len(ngrams)\n",
    "\n",
    "word = \"apple\"\n",
    "custom_embedding = custom_embed(word)\n",
    "gensim_embedding = model.wv[word]\n",
    "\n",
    "print(all(np.isclose(custom_embedding, gensim_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors_ngrams.astype(np.float64).tofile(\"ngrams.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.astype(np.float64).tofile(\"vectors.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"vocab.json\", \"w\") as file:\n",
    "    json.dump(model.wv.key_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
